---
title: Latest 15 Papers - December 04, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## data synthesis
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues](https://arxiv.org/abs/2512.03046v1)** | 2025-12-02 | <details><summary>Code ...</summary><p>Code and demo available at https://magicquill.art/v2/</p></details> |
| **[CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models](https://arxiv.org/abs/2512.03045v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project page: https://cvlab-kaist.github.io/CAMEO/</p></details> |
| **[OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project page: https://github.com/tulerfeng/OneThinker</p></details> |
| **[MultiShotMaster: A Controllable Multi-Shot Video Generation Framework](https://arxiv.org/abs/2512.03041v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project Page: https://qinghew.github.io/MultiShotMaster</p></details> |
| **[Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation](https://arxiv.org/abs/2512.03040v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project page at https://xizaoqu.github.io/video4spatial/</p></details> |
| **[Geometric Modeling of Hippocampal Tau Deposition: A Surface-Based Framework for Covariate Analysis and Off-Target Contamination Detection](https://arxiv.org/abs/2511.01732v2)** | 2025-12-02 |  |
| **[Learning Physically Consistent Lagrangian Control Models Without Acceleration Measurements](https://arxiv.org/abs/2512.03035v1)** | 2025-12-02 | <details><summary>Submi...</summary><p>Submitted to the L4DC 2026</p></details> |
| **[SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control](https://arxiv.org/abs/2512.03028v1)** | 2025-12-02 | 14 pages, 9 figures |
| **[LORE: A Large Generative Model for Search Relevance](https://arxiv.org/abs/2512.03025v1)** | 2025-12-02 |  |

## data selection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues](https://arxiv.org/abs/2512.03046v1)** | 2025-12-02 | <details><summary>Code ...</summary><p>Code and demo available at https://magicquill.art/v2/</p></details> |
| **[OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project page: https://github.com/tulerfeng/OneThinker</p></details> |
| **[MultiShotMaster: A Controllable Multi-Shot Video Generation Framework](https://arxiv.org/abs/2512.03041v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project Page: https://qinghew.github.io/MultiShotMaster</p></details> |
| **[Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation](https://arxiv.org/abs/2512.03040v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project page at https://xizaoqu.github.io/video4spatial/</p></details> |
| **[Geometric Modeling of Hippocampal Tau Deposition: A Surface-Based Framework for Covariate Analysis and Off-Target Contamination Detection](https://arxiv.org/abs/2511.01732v2)** | 2025-12-02 |  |
| **[Learning Physically Consistent Lagrangian Control Models Without Acceleration Measurements](https://arxiv.org/abs/2512.03035v1)** | 2025-12-02 | <details><summary>Submi...</summary><p>Submitted to the L4DC 2026</p></details> |
| **[SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control](https://arxiv.org/abs/2512.03028v1)** | 2025-12-02 | 14 pages, 9 figures |
| **[LORE: A Large Generative Model for Search Relevance](https://arxiv.org/abs/2512.03025v1)** | 2025-12-02 |  |

## MLLMs
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project page: https://github.com/tulerfeng/OneThinker</p></details> |
| **[InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration](https://arxiv.org/abs/2512.02981v1)** | 2025-12-02 | <details><summary>Publi...</summary><p>Published in AAAI 2026</p></details> |
| **[Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities](https://arxiv.org/abs/2512.02973v1)** | 2025-12-02 |  |
| **[Multimodal LLMs See Sentiment](https://arxiv.org/abs/2508.16873v2)** | 2025-12-02 | 12 pages, 7 figures |
| **[MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding](https://arxiv.org/abs/2512.02906v1)** | 2025-12-02 |  |
| **[MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm](https://arxiv.org/abs/2512.02895v1)** | 2025-12-02 | 33 pages, 14 figures |
| **[FiMMIA: scaling semantic perturbation-based membership inference across modalities](https://arxiv.org/abs/2512.02786v1)** | 2025-12-02 | <details><summary>Syste...</summary><p>System demo track paper for EACL 2026</p></details> |
| **[GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding](https://arxiv.org/abs/2512.02715v1)** | 2025-12-02 | 11 pages, 4 figures |
| **[TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding Capabilities of MLLMs](https://arxiv.org/abs/2505.11275v4)** | 2025-12-02 | <details><summary>There...</summary><p>There are issues with the paper</p></details> |
| **[PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding](https://arxiv.org/abs/2512.02624v1)** | 2025-12-02 |  |

## data curation
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues](https://arxiv.org/abs/2512.03046v1)** | 2025-12-02 | <details><summary>Code ...</summary><p>Code and demo available at https://magicquill.art/v2/</p></details> |
| **[OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project page: https://github.com/tulerfeng/OneThinker</p></details> |
| **[MultiShotMaster: A Controllable Multi-Shot Video Generation Framework](https://arxiv.org/abs/2512.03041v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project Page: https://qinghew.github.io/MultiShotMaster</p></details> |
| **[Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation](https://arxiv.org/abs/2512.03040v1)** | 2025-12-02 | <details><summary>Proje...</summary><p>Project page at https://xizaoqu.github.io/video4spatial/</p></details> |
| **[Geometric Modeling of Hippocampal Tau Deposition: A Surface-Based Framework for Covariate Analysis and Off-Target Contamination Detection](https://arxiv.org/abs/2511.01732v2)** | 2025-12-02 |  |
| **[Learning Physically Consistent Lagrangian Control Models Without Acceleration Measurements](https://arxiv.org/abs/2512.03035v1)** | 2025-12-02 | <details><summary>Submi...</summary><p>Submitted to the L4DC 2026</p></details> |
| **[SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control](https://arxiv.org/abs/2512.03028v1)** | 2025-12-02 | 14 pages, 9 figures |
| **[LORE: A Large Generative Model for Search Relevance](https://arxiv.org/abs/2512.03025v1)** | 2025-12-02 |  |
| **[TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models](https://arxiv.org/abs/2511.02802v3)** | 2025-12-02 | <details><summary>The l...</summary><p>The library is open source and available at https://github.com/Lexsi-Labs/TabTune</p></details> |

