---
title: Latest 15 Papers - December 17, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## data synthesis
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Quantum oracles give an advantage for identifying classical counterfactuals](https://arxiv.org/abs/2512.13692v1)** | 2025-12-15 | <details><summary>5+4 p...</summary><p>5+4 pages. Comments welcome!</p></details> |
| **[DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690v1)** | 2025-12-15 | <details><summary>Proje...</summary><p>Project page: https://susunghong.github.io/DiffusionBrowser</p></details> |
| **[Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687v1)** | 2025-12-15 | <details><summary>Our p...</summary><p>Our pre-trained models are available at https://github.com/MiniMax-AI/VTP</p></details> |
| **[Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684v1)** | 2025-12-15 |  |
| **[Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678v1)** | 2025-12-15 | <details><summary>https...</summary><p>https://glab-caltech.github.io/steer3d/</p></details> |
| **[Towards Effective Model Editing for LLM Personalization](https://arxiv.org/abs/2512.13676v1)** | 2025-12-15 | <details><summary>15 pa...</summary><p>15 pages (including appendix), 7 figures. Code, data, results, and additional resources are available at: https://model-editing.github.io</p></details> |

## data selection
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Quantum oracles give an advantage for identifying classical counterfactuals](https://arxiv.org/abs/2512.13692v1)** | 2025-12-15 | <details><summary>5+4 p...</summary><p>5+4 pages. Comments welcome!</p></details> |
| **[Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687v1)** | 2025-12-15 | <details><summary>Our p...</summary><p>Our pre-trained models are available at https://github.com/MiniMax-AI/VTP</p></details> |
| **[Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684v1)** | 2025-12-15 |  |
| **[Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678v1)** | 2025-12-15 | <details><summary>https...</summary><p>https://glab-caltech.github.io/steer3d/</p></details> |
| **[Towards Effective Model Editing for LLM Personalization](https://arxiv.org/abs/2512.13676v1)** | 2025-12-15 | <details><summary>15 pa...</summary><p>15 pages (including appendix), 7 figures. Code, data, results, and additional resources are available at: https://model-editing.github.io</p></details> |
| **[Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions](https://arxiv.org/abs/2505.08919v2)** | 2025-12-15 | <details><summary>Manus...</summary><p>Manuscript accepted by Medical Image Analysis, 2025</p></details> |

## MLLMs
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection](https://arxiv.org/abs/2512.13671v1)** | 2025-12-15 |  |
| **[MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573v1)** | 2025-12-15 | Acceped by AAAI 2026 |
| **[TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511v1)** | 2025-12-15 | <details><summary>18 Pa...</summary><p>18 Pages. Project page at http://bpiyush.github.io/tara-website</p></details> |
| **[HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs](https://arxiv.org/abs/2508.10576v4)** | 2025-12-15 | Accepted by AAAI2026 |
| **[ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303v1)** | 2025-12-15 | <details><summary>proje...</summary><p>project page: https://lntzm.github.io/showtable-page/</p></details> |
| **[ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution](https://arxiv.org/abs/2510.12793v2)** | 2025-12-15 |  |
| **[Do MLLMs Really Understand the Charts?](https://arxiv.org/abs/2509.04457v2)** | 2025-12-15 |  |
| **[SpurLens: Automatic Detection of Spurious Cues in Multimodal LLMs](https://arxiv.org/abs/2503.08884v2)** | 2025-12-15 |  |
| **[Thinking with Visual Abstract: Enhancing Multimodal Reasoning via Visual Abstraction](https://arxiv.org/abs/2505.20164v3)** | 2025-12-15 |  |
| **[DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799v1)** | 2025-12-14 |  |

## data curation
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Quantum oracles give an advantage for identifying classical counterfactuals](https://arxiv.org/abs/2512.13692v1)** | 2025-12-15 | <details><summary>5+4 p...</summary><p>5+4 pages. Comments welcome!</p></details> |
| **[Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687v1)** | 2025-12-15 | <details><summary>Our p...</summary><p>Our pre-trained models are available at https://github.com/MiniMax-AI/VTP</p></details> |
| **[Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684v1)** | 2025-12-15 |  |
| **[Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678v1)** | 2025-12-15 | <details><summary>https...</summary><p>https://glab-caltech.github.io/steer3d/</p></details> |
| **[Towards Effective Model Editing for LLM Personalization](https://arxiv.org/abs/2512.13676v1)** | 2025-12-15 | <details><summary>15 pa...</summary><p>15 pages (including appendix), 7 figures. Code, data, results, and additional resources are available at: https://model-editing.github.io</p></details> |
| **[Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions](https://arxiv.org/abs/2505.08919v2)** | 2025-12-15 | <details><summary>Manus...</summary><p>Manuscript accepted by Medical Image Analysis, 2025</p></details> |

